# ollama

它是一个让你能通过简单命令在本地 轻松下载、运行和管理大语言模型的工具，支持GPU加速和类OPENAI接口，适合本地部署和开发。

# Meta LLama 羊驼模型
deepseek-r1:1.5b 参数的尺寸 
Qwen 

在11434端口提供api调用